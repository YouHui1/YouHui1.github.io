[{"title":"pytorch学习","date":"2022-05-28T14:23:06.000Z","path":"2022/05/28/pytorch学习/","text":"0.简要参考资料： PyTorch深度学习快速入门教程（绝对通俗易懂！）【小土堆】以下为观看视频时做的笔记。 1.工具运用1.1 TensorBoard使用简介： TensorFlow 的可视化工具包开启： 键入命令tensorboard —logdir 日志名称注意： 命令需要在创建日志的目录下键入，或者键入的日志名称需要改为绝对路径 创建日志： 在python环境下创建writer from torch.utils.tensorboard import SummaryWriter writer = SummaryWriter(&quot;logs&quot;) #... writer.close() 例如在图中绘制散点: from torch.utils.tensorboard import SummaryWriter writer = SummaryWriter(&quot;logs&quot;) for i in range(100): writer.add_scalar(&quot;y=2*x&quot;, 3 * i, i) writer.close() 随后在该py文件目录下键入命令tensorboard —logdir logs，会得到一个端口 1.2 Transform使用基本结构： graph LR input-->tools tools-->output 其中Transform起到tools的作用 1.2.1 transforms.ToTensor()作用： 将图片改为tensor类型 from PIL import Image from torchvision import transforms img = Image.open(img_path) trans = transforms.ToTensor() img_tensor = trans(img) 上面代码的结构如下： graph LR img[\"img(PIL)\"]-->trans[\"trans\"] trans[\"trans\"]-->tensor_img[\"img(tensor)\"] 1.2.2 transforms.Normalize()作用： 图片正则化 / 归一化 from torchvision import transforms # print(img_Tensor[0][0][0]) trans_norm = transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]) # 计算公式：input[channel] = (input[channel]（输入） - mean[channel]（均值）) / std[channel]（标准差） img_norm = trans_norm(img_Tensor) # print(img_norm[0][0][0]) 1.2.3 transforms.Resize()作用： 调整图片大小 trans_resize = transforms.Resize((512, 512)) # img PIL -&gt; resize -&gt; img_resize PIL img_resize = trans_resize(img) # img_resize PIL -&gt; toTensor -&gt; img_resize tensor img_resize = trans_toTensor(img_resize) 1.2.4 transforms.Compose()作用： Compose的参数输入一个transfrom操作列表，通过对参数遍历对输入进行操作 trans_toTensor = transforms.ToTensor() trans_resize = transforms.Resize(x) # 将图片短边缩放至x，长宽比保持不变 trans_compose = transforms.Compose([trans_toTensor, trans_resize]) img_compose = trans_compose(img) 以上代码结构如下： graph LR old_img[\"img(PIL)\"]-->trans[\"compose\"] trans[\"compose\"]-->new_img[\"img(tensor)(resize)\"] compose-.->ToTensor ToTensor-.->resize 因此实际上结构为： graph LR old_img[\"img(PIL)\"]-->totansor[\"ToTensor\"] totansor[\"ToTensor\"]-.->resize resize-->new_img[\"img(tensor)(resize)\"] 1.2.5 transforms.RandomCrop()作用： 随机裁剪图片，参数可设置裁剪大小 trans_RandomCrop = transforms.RandomCrop((512, 512)) img_crop = trans_RandomCrop(img) 1.3 torchvision中数据集的使用1.3.1 下载数据集import torchvision dataset_transform = torchvision.transforms.Compose([ torchvision.transforms.ToTensor() ]) train_set = torchvision.datasets.CIFAR10(root=&quot;./dataset2&quot;, train=True, download=True, transform=dataset_transform) test_set = torchvision.datasets.CIFAR10(root=&quot;./dataset2&quot;, train=False, download=True, transform=dataset_transform) # CIFAR10 数据集 # root=数据集存放路径, # train=是否为训练集, # download=是否下载, # transform=将PIL图片以某些函数进行转换加工等 1.3.2 Dataloader使用from torch.utils.data import DataLoader test_loader = DataLoader(dataset=test_data, batch_size=4, shuffle=True, num_workers=0, drop_last=False)","tags":[{"name":"学习","slug":"学习","permalink":"https://youhui1.github.io/tags/%E5%AD%A6%E4%B9%A0/"},{"name":"pytorch","slug":"pytorch","permalink":"https://youhui1.github.io/tags/pytorch/"}]},{"title":"决策树","date":"2022-04-14T04:48:14.000Z","path":"2022/04/14/决策树/","text":"hr:nth-of-type(1) { border-color: black !important; border-width: 3px !important; } hr:nth-of-type(2) { border-color: black !important; border-width: 3px !important; } 1. 基本流程 决策树（decision tree）（判定树）是一类常见的机器学习算法。主要用于对样本的分类。 决策树是基于树的结构进行决策的。 一般的，一颗决策树包含一个根节点、若干个内部节点与若干个叶结点。 叶结点：决策结果。 其他节点：属性测试。 每个结点包含的样本集合根据属性测试的结果被划分到子节点中。 根节点包含样本全集。从根节点到每个叶结点的路径对应了一个判定测试序列。 目的：决策树学习的目的是为了产生一棵泛化能力强，即处理未见事例能力强的决策树。 采取策略：分而治之（divide-and-conquer）。 伪代码如下： 输入： 训练集 D={(x1,y1),(x2,y2),…,(xm,ym)};D=\\{(\\boldsymbol{x}_1,y_1),(\\boldsymbol{x}_2,y_2),\\dots,(\\boldsymbol{x}_m,y_m)\\};D={(x1​,y1​),(x2​,y2​),…,(xm​,ym​)}; 属性集 A={a1,a2,…,ad}.A=\\{a_1,a_2,\\dots,a_d\\}.A={a1​,a2​,…,ad​}. 过程： 函数 TreeGenerate(D,A)TreeGenerate(D,A)TreeGenerate(D,A) 1: 生成结点 nodenodenode; 2: if D\\boldsymbol{if}\\ \\ Dif D 中样本全属于同一类别 C thenC\\ \\ \\boldsymbol{then}C then 3: 将 nodenodenode 标记为 CCC 类叶结点; return\\ \\boldsymbol{return} return 4: end if\\boldsymbol{end \\ \\ if}end if 5: if A=∅ OR D\\boldsymbol{if}\\ \\ A=\\varnothing \\ \\ \\boldsymbol{OR}\\ \\ Dif A=∅ OR D 样本在 AAA 上取值相同 then\\ \\ \\boldsymbol{then} then 6: 将 nodenodenode 标记为 CCC 类叶结点,其类别标记为 DDD 中样本数最多的类; return\\ \\boldsymbol{return} return 7: end if\\boldsymbol{end \\ \\ if}end if 8: 从 AAA 中选择最优划分属性 a∗a_*a∗​; 9: for a∗\\boldsymbol{for}\\ \\ a_*for a∗​ 的每一个值 a∗v doa_*^v\\ \\ \\boldsymbol{do}a∗v​ do 10: 为 nodenodenode 生成一个分支;令 DvD_vDv​ 表示DDD 中在 a∗a_*a∗​ 上取值为 a∗va_*^va∗v​ 的样本子集; 11: if Dv\\boldsymbol{if}\\ \\ D_vif Dv​ 为空 then\\ \\boldsymbol{then} then 12: 将分支结点标记为叶结点,其类别标记为 DDD 中样本最多的类; return\\ \\boldsymbol{return} return 13: else\\boldsymbol{else}else 14: 以 Treegenerate(Dv,A∖{a∗})Treegenerate(D_v,A \\setminus \\{ a_* \\})Treegenerate(Dv​,A∖{a∗​}) 为分支结点 \\quad 15: end if\\boldsymbol{end \\ \\ if}end if 16: end for\\boldsymbol{end \\ \\ for}end for 输入： 以 nodenodenode 为根节点的一棵决策树 三个 ififif 对应三个终止条件： DDD 已经分类完毕，无须继续进行 DDD 中所有元素在 AAA 上取值相同，说明没办法区分开来了，无法继续进行，取类别标记为 DDD 中样本数最多的类（如105个样本，100个A类，5个B类 →\\rightarrow→ 标记为A类） 当前结点包含的样本集 Dv(Dv⊆D)D_v (D_v \\sube D)Dv​(Dv​⊆D) 为空，无法划分，取类别标记为 DDD 中样本数最多的类（如D有105个样本，100个A类，5个B类，没有C类，但根据属性划分出C类子结点的样本数为0 →\\rightarrow→ 就把C类子结点类别设定为D所含样本最多的类别A类）（当无样本了就返回分类前情况？） 参考资料： 一起啃西瓜书(4)-决策树《机器学习-周志华》 2. 划分选择 决策树学习关键在于选择最优的划分属性，使得决策树的分支结点所包含的样本尽可能属于同一类别，即结点的纯度（purity）越来越高。 2.0 熵、信息 *数据 = 噪音 + 信息 熵：一种事物的不确定性 熵的量化 参照一个不确定的事件作为单位 例：抛一次硬币50%正，50%反，记为1bit 抛硬币次数与结果不确定性呈指数关系 等概率均匀分布：n=log⁡2mn = \\log_2mn=log2​m 例：8个等概率不确定情况，熵为3bit 若 m=10m = 10m=10，那么10=2n,n=log⁡21010 = 2^n,n = \\log_21010=2n,n=log2​10 每种情况概率不相等，一般分布 2.1 信息增益（information gain） “信息熵”（information entropy）是度量样本集合纯度最常用的一种指标。假定当前样本集合 DDD 中第 kkk 类样本所占的比例为 pk(k=1,2,...,∣Y∣)p_k(k=1,2,...,|\\mathcal{Y}|)pk​(k=1,2,...,∣Y∣)，则 DDD 的信息熵定义为$$Ent(D)=\\sum_{k=1}^{|\\mathcal{Y}|}p_k\\log_2\\frac{1}{p_k}\\Rightarrow Ent(D)=-\\sum_{k=1}^{|\\mathcal{Y}|}p_k\\log_2p_k$$ Ent(D)Ent(D)Ent(D) 的值越小，则 DDD 的纯度越高。 上述公式用于一般分布中 例：4类样本A,B,C,D原本各自所占比例均分，pk=0.25,k=A,B,C,Dp_k = 0.25, k = A,B,C,Dpk​=0.25,k=A,B,C,D ，计算信息熵 n=log⁡2m,n=2bitsn = \\log_2m, n = 2 bitsn=log2​m,n=2bits ，后续条件更改，C占比1/2，其余占比均分，为1/6，通过信息熵计算公式计算 \\begin{equation*} Ent(D) = \\frac{1}{6}\\log_26 +\\frac{1}{6}\\log_26 +\\frac{1}{2}\\log_22 +\\frac{1}{6}\\log_26 = 1.79 \\end{equation*} 假设离散属性 aaa 有 VVV 个可能取值 {a1,a2,…,aV}\\{a^1,a^2,\\dots,a^V\\}{a1,a2,…,aV} ，若以 aaa 对样本进行划分样本集 DDD 时，会产生 VVV 个分支结点，其中第 vvv 个分支结点包含 DDD 中所有在属性 aaa 上取值为 ava^vav 的样本，记为 DvD^vDv ，可计算 DvD^vDv 的信息熵，再赋予权重 ∣Dv∣/∣D∣\\lvert D^v \\lvert / \\lvert D \\lvert∣Dv∣/∣D∣ ，计算信息增益： \\begin{equation*} Gain(D,a) = Ent(D) - \\sum^V_{v=1}\\frac{\\lvert D^v \\lvert}{\\lvert D \\lvert} Ent(D^v) \\end{equation*} 例：样本集 DDD 根节点的信息熵为 Ent(D)Ent(D)Ent(D) ，有四个属性 A,B,C,DA,B,C,DA,B,C,D ，计算它们的信息增益 Gain(D,?)(?=A,B,C,D)Gain(D, ?)(? = A,B,C,D)Gain(D,?)(?=A,B,C,D) ，取信息增益最大值对应的属性进行划分。假设划分成3部分，随后将3个部分作为根节点继续以属性 B,C,DB,C,DB,C,D 进行划分，算出信息增益 Gain(Di,?)(i=1,2,3;?=B,C,D)Gain(D^i, ?)(i = 1,2,3; ? = B,C,D)Gain(Di,?)(i=1,2,3;?=B,C,D) 以此类推。 2.2 增益率（gain ratio） 由于信息增益准则对可取值数目较多的属性有偏好，为减小这种带来的不利影响，采用增益率来选择划分最优属性。定义： \\begin{equation*} Gain\\_ ratio(D,a) = \\frac{Gain(D,a)}{IV(a)} \\end{equation*} 其中 \\begin{equation*} IV(a) = \\sum^V_{v=1} \\frac{\\lvert D^v \\lvert}{\\lvert D \\lvert} \\log_2\\frac{\\lvert D^v \\lvert}{\\lvert D \\lvert} \\end{equation*} 用法：先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。 2.3 基尼指数（Gini index） 数据集 DDD 的纯度用基尼值度量： \\begin{equation*} Gini(D) = \\sum^{\\lvert \\mathcal{Y} \\lvert}_{k=1}\\sum_{k^\\prime\\neq k}p_kp_{k^\\prime} = 1-\\sum^{\\lvert \\mathcal{Y} \\lvert}_{k=1}p_k^2 \\end{equation*} Gini(D)Gini(D)Gini(D) 反映了从数据集 DDD 中随机抽取两个样本类别标记不一样的概率，因此 Gini(D)Gini(D)Gini(D) 越小，数据集 DDD 的纯度越高 属性 aaa 的基尼指数定义(加权平均)： GiniIndex(D,a)=∑v=1V∣Dv∣DGini(Dv)GiniIndex(D,a)=\\sum_{v=1}^V\\frac{|D^v|}{D}Gini(D^v) GiniIndex(D,a)=v=1∑V​D∣Dv∣​Gini(Dv) 所以以基尼指数选择属性划分时，所选的属性满足：a∗=argmina∈A GiniIndex(D,a)a_*=\\underset{a\\in A}{argmin} \\ GiniIndex(D, a)a∗​=a∈Aargmin​ GiniIndex(D,a) 12345678910def gini_index_single(a, b): single_gini = 1 - ((a/(a+b))**2) - ((b/(a+b))**2) return round(single_gini, 3)def gini_index(a, b, c, d): left = gini_index_single(a, b) right = gini_index_single(c, d) #加权平均 gini_index = left*((a+b)/(a+b+c+d)) + right*((c+d)/(a+b+c+d)) return round(gini_index, 3) graph TD 结果.好/坏. 因素1--是-->1(结果.105/39.); 因素1--否-->2(结果.34/125.); 因素2--是-->3(结果.37/127.); 因素2--否-->4(结果.100/33.); 因素3--是-->5(结果.92/31.); 因素3--否-->6(结果.45/129.); 12345#第一次分支，选择结点，计算GiniIndex，取最小值gini_index(105,39,34,125) -&gt; 0.364gini_index(37,127,100,33) -&gt; 0.36gini_index(92,31,45,129) -&gt; 0.381#选取因素2 假设第一次分配后现象如下，目前以37/127结点继续选择分支举例 graph TD 结果.好/坏. 因素2-->37/127; 因素2-->100/33; 37/127-.->因素1 37/127-.->因素3 因素1--是-->1(结果.13/98.); 因素1--否-->2(结果.24/29.); 因素3--是-->5(结果.24/25.); 因素3--否-->6(结果.13/102.); 12345678#计算37/127本身的Gini系数gini_index_single(37, 127) -&gt; 0.349#再计算因素Gini值，取最小gini_index(13,98,24,29) -&gt; 0.3gini_index(24,25,13,102) -&gt; 0.29#0.29 &lt; 0.349#选取因素3 graph TD 结果.好/坏. 因素2-->因素3; 因素2-->100/33; 因素1--是-->1(结果.1/10.); 因素1--否-->2(结果.12/92.); 因素3-->24/25; 因素3-->13/102; 13/102-.->因素1; 继续计算，以13/102结点举例： 1234567#计算13/102本身的Gini指数gini_index_single(13, 102) -&gt; 0.201#再计算因素Gini值，取最小gini_index(1,10,12,92) -&gt; 0.201#0.201=0.201#在13/102结点处终止 graph TD 结果.好/坏. 因素2-->因素3; 因素2-->100/33; 因素1--是-->1(结果.1/10.); 因素1--否-->2(结果.12/92.); 因素3-->24/25; 因素3-->13/102; 2.4 平方误差最小准则（Sum of Squared Residuals SSR） 生成回归树时对于分支条件的选择需要通过SSR来筛选。 对于一个特征的决策树，求取分支条件阈值的流程： 阈值从小到大移动； 每移动一次，计算阈值左右两边的平方误差； 计算后取平方误差最小的阈值作为分支阈值。 对于多个特征的决策树，计算方法类似，不同之处在于需要对多个特征的SSR做比较，随后选择SSR最小的特征作为分支结点。 防止过拟合（分支过多）的举措为设定叶结点的数目上限，视情况而定，一般20个？ 2.5 剪枝（pruning） 目的：防止过拟合。 策略：预剪枝（prepruning）和后剪枝（postpruning） 预剪枝（能剪则剪）（自上而下？）：选择属性分支，若分支后 验证集精度不大于分支之前的精度，则预剪枝策略禁止该节点划分。 后剪枝（能不剪就不剪）（自下而上？）：从分支后的验证集精度看起，倘若没有该分支的验证集精度更大，则进行剪枝。","tags":[{"name":"学习","slug":"学习","permalink":"https://youhui1.github.io/tags/%E5%AD%A6%E4%B9%A0/"},{"name":"机器学习","slug":"机器学习","permalink":"https://youhui1.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"梯度下降","date":"2022-04-10T14:02:09.000Z","path":"2022/04/10/梯度下降/","text":"问题概述 存在一个函数 J(θ0,θ1)J(\\theta_0, \\theta_1)J(θ0​,θ1​) (J(θ0,θ1,...))( J(\\theta_0, \\theta_1,...))(J(θ0​,θ1​,...)) 需要实现 minθ0,θ1 J(θ0,θ1)\\quad\\underset{\\theta_0, \\theta_1}{min}\\ J(\\theta_0, \\theta_1)θ0​,θ1​min​ J(θ0​,θ1​)(minθ0,θ1,... J(θ0,θ1,...))(\\underset{\\theta_0, \\theta_1,...}{min}\\ J(\\theta_0, \\theta_1,...))(θ0​,θ1​,...min​ J(θ0​,θ1​,...)) 一般初始设置两个参数的值为0，通过不断改变两个参数的值使得整个函数的值最小，直到找到最小值（整体/局部）。 梯度下降算法（Gradient descent algorithm） 重复下式直至收敛(convergence)： \\begin{equation*} \\theta_j:=\\theta_j - \\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta_0,\\theta_1)\\quad(for\\ \\ j = 0\\ \\ and\\ \\ j = 1) \\end{equation*} α\\alphaα：学习率(learning rate)，决定梯度下降的步长。 学习率太小（欠拟合）（左），最小值较难找到。 学习率过大（过拟合）（右），难以收敛。 算法实现过程： temp0:=θ0−α∂∂θ0J(θ0,θ1)temp0:=\\theta_0 - \\alpha\\frac{\\partial}{\\partial\\theta_0}J(\\theta_0,\\theta_1)temp0:=θ0​−α∂θ0​∂​J(θ0​,θ1​) temp1:=θ1−α∂∂θ1J(θ0,θ1)temp1:=\\theta_1 - \\alpha\\frac{\\partial}{\\partial\\theta_1}J(\\theta_0,\\theta_1)temp1:=θ1​−α∂θ1​∂​J(θ0​,θ1​) θ0:=temp0\\theta_0:=temp0θ0​:=temp0 θ1:=temp1\\theta_1:=temp1θ1​:=temp1 注意，赋值顺序不颠倒（指(12)与(34)） 线性回归的梯度下降算法 \\begin{equation*} h_{\\theta}(x)=\\theta_0+\\theta_1x \\end{equation*} \\begin{equation*} J(\\theta_0,\\theta_1)=\\frac{1}{2m}\\sum^m_{i=1}(h_{\\theta}(x^{(i)})-y^{(i)})^2 \\end{equation*} （x(i)x^{(i)}x(i)是一个向量）（向量一般指列向量，在此为1*1向量）（JJJ为代价函数） 对应求偏导 \\begin{equation*} j=0:\\frac{\\partial}{\\partial\\theta_0}J(\\theta_0,\\theta_1)=\\frac{1}{m}\\sum^m_{i=1}(h_{\\theta}(x^{(i)})-y^{(i)}) \\end{equation*} \\begin{equation*} j=1:\\frac{\\partial}{\\partial\\theta_1}J(\\theta_0,\\theta_1)=\\frac{1}{m}\\sum^m_{i=1}(h_{\\theta}(x^{(i)})-y^{(i)})x^{(i)} \\end{equation*} 代入公式，调整参数 \\begin{equation*} \\theta_j:=\\theta_j - \\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta_0,\\theta_1)\\quad(for\\ \\ j = 0\\ \\ and\\ \\ j = 1) \\end{equation*} 在线性模型中，此梯度下降算法遍历整个训练集 多元梯度下降法 符号解释 nnn —— 特征数量 mmm —— 样本个数 x(i)x^{(i)}x(i) —— 第i个训练样本的特征向量（the vector of features） xj(i)x^{(i)}_jxj(i)​ —— 第i个训练样本的第j个特征值（the value of the feature） 原先算法 假设（n = 1）时， 重复步骤 \\begin{equation*}\\theta_j:=\\theta_j - \\alpha\\frac{\\partial}{\\partial\\theta_j}J(\\theta_0,\\theta_1)\\quad(for\\ \\ j = 0\\ \\ and\\ \\ j = 1)\\end{equation*} 即 \\begin{equation*}j=0:\\frac{\\partial}{\\partial\\theta_0}J(\\theta_0,\\theta_1)=\\frac{1}{m}\\sum^m_{i=1}(h_{\\theta}(x^{(i)})-y^{(i)})\\end{equation*} \\begin{equation*}j=1:\\frac{\\partial}{\\partial\\theta_1}J(\\theta_0,\\theta_1)=\\frac{1}{m}\\sum^m_{i=1}(h_{\\theta}(x^{(i)})-y^{(i)})x^{(i)}\\end{equation*} 新算法 （n &gt;= 1）时， 重复步骤 \\begin{equation*}\\theta_j:=\\theta_j - \\alpha\\frac{1}{m}\\sum^m_{i=1}(h_{\\theta}(x^{(i)})-y^{(i)})x^{(i)}_j\\quad(for\\ \\ j = 0,1,2,...,n)\\end{equation*} 默认x0(i)=1x^{(i)}_0 = 1x0(i)​=1 缺点：需要计算每个维度的值，计算量大 随机梯度算法 随机选择一个方向计算 缺点：达到最小值时不一定会停止，得到的参数非最优，只能说比较好。 补充 特征缩放（Feature Scaling） 目的：确保特征值在相近的范围内 缩放范围控制接近−1≤x≤1-1 \\le x \\le 1−1≤x≤1 均值归一化 一般不对x0=1x_0 = 1x0​=1使用 公式： \\begin{equation*}x_i = \\frac{x_i - \\mu_i}{s_i}\\quad(\\mu_i = average(x),\\ s_i = max(range(x)))\\end{equation*}","tags":[{"name":"学习","slug":"学习","permalink":"https://youhui1.github.io/tags/%E5%AD%A6%E4%B9%A0/"},{"name":"机器学习","slug":"机器学习","permalink":"https://youhui1.github.io/tags/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"}]},{"title":"线性模型","date":"2022-04-01T12:37:03.000Z","path":"2022/04/01/线性模型/","text":"线性回归1. 基本形式\\begin{equation*}f(\\boldsymbol x) = w_1x_1 +w_2x_2 + ... + w_dx_d + b\\end{equation*}向量形式： \\begin{equation*}f(\\boldsymbol x) = \\boldsymbol w^T\\boldsymbol x + b\\end{equation*}其中 \\begin{equation*}\\boldsymbol x = (x_1;x_2;...x_d;)\\end{equation*}\\begin{equation*}\\boldsymbol w = (w_1;w_2;...w_d;)\\end{equation*}$x_i$ 为 $\\boldsymbol x$ 的第 $i$ 个属性取值，$\\boldsymbol w$ 为权重。目的为通过算法学习确定 $\\boldsymbol w$ 与 $b$ 的值，进而确定模型。 2. 一元线性回归给定数据集 \\begin{equation*}D=\\{(\\boldsymbol x_1,y_1), (\\boldsymbol x_2,y_2),...,(\\boldsymbol x_m,y_m)\\} = \\{(\\boldsymbol x_i,y_i)\\}^m_{i=1}\\end{equation*}试图学得 $\\boldsymbol w$ 与 $b$ $(\\underset{md}{\\boldsymbol x},\\underset{1m}{\\boldsymbol w},\\underset{m1}{b},\\underset{m1}{y})$令 \\begin{equation*}f(x_i) = w_ix_i+b \\quad s.t. \\quad f(x_i)≃y\\end{equation*}此时$w$与$b$的取值要满足： \\begin{equation*} \\begin{aligned} (w^*,b^*)&= \\mathop {argmin}\\limits_{(w,b)} \\sum^m_{i=1}(f(x)-y_i)^2 \\\\ &= \\mathop {argmin}\\limits_{(w,b)} \\sum^m_{i=1}(y_i-wx_i-b)^2 \\end{aligned} \\end{equation*}即所选择的两个参数使得均方误差最小，上面则为目标函数。求解：最小二乘法，对$w$与$b$求导，令$E_{(w,b)}=\\sum_{i=1}^m(y_i-wx_i-b)^2$ \\begin{equation*} \\begin{aligned} \\left.\\frac{\\partial E(w,b)}{\\partial w}\\right. &= 2(w\\sum^m_{i=1}x^2_i-\\sum^m_{i=1}(y_i-b)x_i) \\\\ \\left.\\frac{\\partial E(w,b)}{\\partial b}\\right. &= 2(mb-\\sum^m_{i=1}(y_i-wx_i)) \\end{aligned} \\end{equation*}令以上两式等于0，得 \\begin{equation*} \\begin{aligned} w &= \\frac{\\sum^m_{i=1}y_i(x_i- \\overline x)}{\\sum^m_{i=1}x_i^2-\\frac{1}{m}(\\sum^m_{i=1}x_i)^2} \\\\ b &= \\frac{1}{m}(\\sum^m_{i=1}y_i-wx_i) \\end{aligned} \\end{equation*} 3. 多元线性回归关系式：\\begin{equation*}f(\\boldsymbol{x_i}) = \\boldsymbol{w^Tx_i} +b \\quad s.t. \\quad f(\\boldsymbol{x_i})≃y\\end{equation*}可继续利用最小二乘法对两个参数进行估计，但需要做些改变，令$\\hat{\\boldsymbol{w}} = (\\boldsymbol{w};b)$,$D$表示为一个$m\\times(d+1)$大小的矩阵$\\boldsymbol{X}$,$\\boldsymbol{X}$最后一列设为1。 \\boldsymbol{X} = \\begin{bmatrix} x_{11}&x_{12}&...&x_{1d}&1 \\\\ x_{21}&x_{22}&...&x_{2d}&1 \\\\ ...&...&...&...&...\\\\ x_{m1}&x_{m2}&...&x_{md}&1 \\end{bmatrix}= \\begin{bmatrix} \\boldsymbol{x_{1}^T}&1 \\\\ \\boldsymbol{x_{2}^T}&1 \\\\ ...&...\\\\ \\boldsymbol{x_{m}^T}&1 \\end{bmatrix}将$y$写成向量形式$\\boldsymbol{y}=(y_1;y_2;…;y_m)$有 \\begin{aligned} \\hat{\\boldsymbol{w}}^*=\\mathop{argmin}\\limits_{\\hat{\\boldsymbol{w}}}(\\boldsymbol{y-X\\hat{w}})^T(\\boldsymbol{y-X\\hat{w}}) \\end{aligned}求导，当$\\boldsymbol{X}^T\\boldsymbol{X}$为满秩矩阵或正定矩阵时候，令求导结果为0得 \\hat{\\boldsymbol{w}}^*=(\\boldsymbol{X}^T\\boldsymbol{X})^{-1}\\boldsymbol{X}^T\\boldsymbol{y}#上式代码： x = np.random.rand(d, 1) y = b + w * x + np.random.randn(d, 1) X = np.c_[np.ones((d, 1), x)] theta_best = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y) 4. 对数线性回归y=\\boldsymbol{w^Tx} +b\\Rightarrow lny=\\boldsymbol{w^Tx} +b 5. 广义线性模型考虑单调可微函数$g(\\cdot)$（联系函数） y=g^{-1}(\\boldsymbol{w^Tx} +b)广义线性模型GLM(Generalized Linear Models)的定义由两部分组成： 一个分布函数，刻画因变量$y$的分布：假设$y$的值域是$\\R$，可以选择正态分布；假设$y$的值域是$\\R^+$，可以选择Gamma分布；假设$y$的值域是$\\{0,1\\}$，可以选择Bernoulli分布；假设$y$的值域是$\\N$，可以选择Poisson分布——只需要假设$y$属于指数分布族(exponential family distribution) 联系/连接函数$g$，g(y)=\\boldsymbol{w^Tx} +b 对数几率回归（逻辑回归）1. 单位阶跃函数（unit-step function） y=\\left\\{\\begin{aligned} 0, \\quad z < 0 \\\\ 0.5, \\quad z = 0 \\\\ 1, \\quad z > 0 \\\\ \\end{aligned}\\right. 2. 对数几率函数（surrogate function）（一种sigmoid函数）\\begin{aligned} y=\\frac{1}{1+e^{-(\\boldsymbol{w^Tx} +b)}}\\rightarrow \\\\ \\\\ \\ln\\frac{y}{1-y}=\\boldsymbol{w^Tx} +b \\end{aligned}解释：假设$y$值为概率，$y = f(x)$,其中， $\\\\y\\in[0,1],$ $y\\rightarrow0,1$时，$\\frac{\\Delta y}{\\Delta x}\\rightarrow0$ 按照广义线性回归的定义，希望找到一个连接函数$g$，使得\\begin{equation*}g(y) = \\boldsymbol{w^Tx} +b\\ (\\sum^k_{i=0}w_ix_i,x_0=1,w_0=b)\\end{equation*}对于该函数，有 $\\frac{\\Delta g}{\\Delta y}\\rightarrow\\infty$，$\\frac{\\Delta g}{\\Delta y}$ 与 $\\frac{1}{(1-y)y}$成正相关，假设\\begin{equation*}\\frac{\\Delta g}{\\Delta y}=\\frac{1}{(1-y)y}\\end{equation*}两边积分，有\\begin{equation*}g(y)=\\ln\\frac{y}{1-y}\\end{equation*}可求得\\begin{equation*}y=\\frac{e^g}{1 + e^g}=\\frac{1}{1 + e^{-g}}\\end{equation*} \\begin{equation*}g=\\boldsymbol{w^Tx}+b\\end{equation*} 类别不平衡问题若$\\frac{y}{1-y}&gt;1$（即$y&gt;0.5$时）则预测为正例若$\\frac{y}{1-y}&gt;\\frac{m^+}{m^-}$（$m^+$正例数，$m^-$反例数）则预测为正例分类器基于一式决策，要对其预测值进行调整，使得实际上在执行二式，需要令 \\begin{equation*}\\frac{y^\\prime}{1-y^\\prime}=\\frac{y}{1-y}\\times\\frac{m^-}{m^+}\\end{equation*}","tags":[{"name":"学习","slug":"学习","permalink":"https://youhui1.github.io/tags/%E5%AD%A6%E4%B9%A0/"}]},{"title":"水晶球","date":"2022-03-20T05:30:58.000Z","path":"2022/03/20/水晶球/","text":"这是一个水晶球，水晶球里面是一个完整的世界。处于水晶球里面的人们过着自己的忙碌生活，大体上与我所处的世界差不多。 我是谁？这并不重要。反正我只是这颗水晶球的观察者，知晓水晶球里发生的一切。 当然，不知道在什么时候，我瞥见了水晶球里的他。他当时正呆呆地望着那个世界的天空，随后，他不由自主自问道：“我是谁？”这不是很简单嘛，我就是我呗。仅仅只是一瞬间，他似乎也以“我就是我”作为答案，回到了日常生活当中。之后一切如常。 真的一切如常吗？其实不然，在之后我又看见了他。他打算探索这个世界，寻找他心中追求的世界的真实面目。之后，整个世界似乎也随着他这个决定悄悄改变了，实际上是我把更多的注意力集中在他的身上，因为在此时此刻，知晓水晶球的一切的我居然不清楚他心中所想。 本来并不是这样的。只不过在面对现实的时候，他慢慢地觉察到了一些“裂隙”，他其实有些反感，感到一点点不适。但因为裂隙若隐若现，所以他还是选择无视。但是在某一天，他深信不疑的某种秩序产生了裂隙，并且影响到了他的生活，他感到痛苦，接受不了，更是感到震惊，再也不能无视裂隙的存在。于是，他希望去能够找到这个世界的真相，从而“缝合”这个裂隙。 裂隙当然是存在的。倒不如说，裂隙应当存在，这个水晶球处处充满着裂隙。正因为有了这些裂隙的堆积，才组成了这个五彩缤纷的世界，裂隙在水晶球中可是代表一种奇迹。倘若没有这种裂隙，整个世界是完美无缺的，那么整个世界的所有事物都是依照着某种完美无缺的秩序运行演化，那对于这个世界本身来说又有什么存在的意义呢？ 其实，对于他这种决定我是无所谓的。但是我还是希望他能够回到他所处的现实中去，毕竟不好好面对现实谈什么探索真相，毕竟也不知道他会做出什么，说不定最终会导致整个水晶球爆炸，那可不行。于是我尝试去暗示他、去帮助他，让他去以面对现实的方式去探索世界。就像一束火光一样，从自身出发，逐步点亮现实的每一个角落。最终，整个水晶球被点亮，他就会看到整个水晶球样貌，即世界的真相。他似乎也接受到这一暗示，开始按我的想法去做。 不过进展似乎有些不对劲，通过面对现实，他察觉到了越来越多的裂隙，这确实是难以避免的，但他真的难以接受。到后来，他放弃了，内心蒙上了一层阴影。我再也无法干涉他的行动，只能默默地注视着。 放弃了这一方法的他不知道从什么时候他发现了一本魔法书，上面阐明了制造水晶球的方法：以自身的抽象能力构建出一个世界。他很高兴，想着可以通过构建内心世界的方式进而理解这个世界，这个就是他想要的，并且自认为很有天赋。于是他就往制造水晶球这个方向前进了。不知道又过了多少天，他终于把这个水晶球制造出来了。可是水晶球里面的并非一个世界，而是一片混沌。 这对于我来说倒不是一个意外，虽然现在看不懂他在想什么，但是我还是了解他的。他本来就不爱学习，不求甚解。拿到那本魔法书时，他其实是当成工具书来用的，大体只了解制作过程，并没有刻苦钻研过程与过程之间的原理联系、发生机制，而且粗略看完流程就把书扔了。制造出来的水晶球外表有模有样，但实际上可差太远了。 他显然也意识到了这一点，很生气，一气之下把水晶球砸碎了，书也找不到了，他所作的努力成了徒劳。这时他也意识到面对现实的确是最简单有效的办法。可当他回过头再面对这个现实的时候，他所处的现实的每一处裂隙更大了，现实早已四分五裂。 虽然裂隙变大了，但现实本身并没有四分五裂，问题在于那个水晶球上。水晶球的内在物需要依靠自己内心的抽象能力创造，同时水晶球外壳起到抑制内心想象无限增殖的作用。当他把水晶球打破时，内心的想象得以释放。这种想象将现实的实在物覆盖起来，或者取代。对于他来说，幻想与现实的境界随水晶球的打破而打破，他现在面对的现实实际上被他想象世界所覆盖。想象会在各方面无限增殖，因为他难以接受负面的事物，导致他一直看到了“现实”的负面，并且不断地扩大化。这是他感到痛苦，痛苦本身也在不断的扩大。最终，他崩溃了。 其实，要想解决这个问题很简单，还是面对现实。他其实也有些明白，现实本身也掺入了他的无意义的有害想象，裂隙也并没有想象的那么大，没有他想象的那样危险。但内心感悟终究是内心感悟。他没有行动，因为他感到畏惧。他的内心感悟与现实行动终究因想象而产生裂隙。或者说，他有行动，他的行动是与自己的想象进行无意义的搏斗，已经无暇顾及什么面对现实了。他逐渐感觉整个世界均以自己为敌，并且敌人越来越多。但他后来惊讶的发现，敌军的主要领导者就是他自己。 正是这种感觉，他开始抹除与世界的所有联系，一层层关系抹除，他似乎觉察到自己的头脑愈发清晰，其实那也是他的想象。当他抹除与世界的最后一丝联系的瞬间，整个世界必然会将他一并抹除。 真是可悲！对于他来说，只要迈出那一步就可以了，可为什么不做呢？只能说是太可惜了，追寻世界真相失败的他，到头来都不知道“我就是我”的意义。","tags":[{"name":"随记","slug":"随记","permalink":"https://youhui1.github.io/tags/%E9%9A%8F%E8%AE%B0/"}]}]